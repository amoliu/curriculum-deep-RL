{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scriptinit\n",
    "import numpy as np\n",
    "import unittest\n",
    "import numpy.testing\n",
    "import os\n",
    "import theano\n",
    "import cPickle as pickle\n",
    "from agent import DQN, RecurrentReinforceAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Chain MDP from https://github.com/spragunr/deep_q_rl and modified to fit our DQN and API\n",
    "\n",
    "##\n",
    "# For now, the DQN assumes vector representation of state. Modify the state representation here to\n",
    "# get tensor representation later.\n",
    "##\n",
    "class ChainMDP(object):\n",
    "    \"\"\"Simple markov chain style MDP.  Three \"rooms\" and one absorbing\n",
    "    state. States are encoded for the q_network as arrays with\n",
    "    indicator entries. E.g. [1, 0, 0, 0] encodes state 0, and [0, 1,\n",
    "    0, 0] encodes state 1.  The absorbing state is [0, 0, 0, 1]\n",
    "\n",
    "    Action 0 moves the agent left, departing the maze if it is in state 0.\n",
    "    Action 1 moves the agent to the right, departing the maze if it is in\n",
    "    state 2.\n",
    "\n",
    "    The agent receives a reward of .7 for departing the chain on the left, and\n",
    "    a reward of 1.0 for departing the chain on the right.\n",
    "\n",
    "    Assuming deterministic actions and a discount rate of .5, the\n",
    "    correct Q-values are:\n",
    "\n",
    "    .7|.25,  .35|.5, .25|1.0,  0|0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, success_prob=1.0, reward_left=0.7, reward_right=1.0):\n",
    "        self.num_actions = 2\n",
    "        self.num_states = 5\n",
    "        self.gamma = .5\n",
    "        self.success_prob = success_prob\n",
    "\n",
    "        self.actions = [np.array([[0]], dtype='int32'),\n",
    "                        np.array([[1]], dtype='int32')]\n",
    "\n",
    "        self.reward_zero = 0 \n",
    "        self.reward_left = reward_left\n",
    "        self.reward_right = reward_right\n",
    "\n",
    "        self.states = []\n",
    "        for i in range(self.num_states):\n",
    "            self.states.append(np.zeros((self.num_states, 1),\n",
    "                                        dtype=theano.config.floatX))\n",
    "            self.states[-1][i, 0] = 1\n",
    "\n",
    "    def act(self, state, action_index):\n",
    "\n",
    "        \"\"\"\n",
    "        action 0 is left, 1 is right.\n",
    "        \"\"\"\n",
    "        state_index =  np.nonzero(state[:, 0])[0][0]\n",
    "        \n",
    "        if state_index == self.num_states - 1: # terminal state\n",
    "            return self.reward_zero, self.states[-1], np.array([[True]])\n",
    "        \n",
    "        if state_index == self.num_states - 2: # first time in absorbing state\n",
    "            return self.reward_zero, self.states[-1], np.array([[False]])\n",
    "\n",
    "        next_index = state_index\n",
    "        if np.random.random() < self.success_prob:\n",
    "            next_index = state_index + action_index * 2 - 1\n",
    "\n",
    "        # Exit left\n",
    "        if next_index == -1:\n",
    "            return self.reward_left, self.states[-2], np.array([[False]])\n",
    "\n",
    "        # Exit right\n",
    "        if next_index == self.num_states - 2:\n",
    "            return self.reward_right, self.states[-2], np.array([[False]])\n",
    "\n",
    "        if np.random.random() < self.success_prob:\n",
    "            return (self.reward_zero,\n",
    "                    self.states[state_index + action_index * 2 - 1],\n",
    "                    np.array([[False]]))\n",
    "        else:\n",
    "            return (self.reward_zero, self.states[state_index],\n",
    "                    np.array([[False]]))\n",
    "    \n",
    "    def get_state_dimension(self):\n",
    "        return self.num_states\n",
    "    \n",
    "    def get_num_actions(self):\n",
    "        return self.num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####\n",
    "# Unit test for the Deep Q-Network\n",
    "####\n",
    "\n",
    "class DQNTest(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.mdp = ChainMDP()\n",
    "\n",
    "    def all_q_vals(self, net):\n",
    "        \"\"\" Helper method to get the entire Q-table \"\"\"\n",
    "\n",
    "        q_vals = np.zeros((self.mdp.num_states, self.mdp.num_actions))\n",
    "        for i in range(self.mdp.num_states):\n",
    "            q_vals[i, :] = net.fprop(self.mdp.states[i].T)\n",
    "        return q_vals\n",
    "\n",
    "    def train(self, net, steps):\n",
    "        mdp = self.mdp\n",
    "        curr_state = mdp.states[np.random.randint(0, mdp.num_states-1)]\n",
    "        for step in xrange(steps):\n",
    "            action = net.get_action(curr_state)\n",
    "            reward, next_state, terminal = mdp.act(curr_state, action)\n",
    "            if terminal:\n",
    "                net.end_episode(reward)\n",
    "                curr_state = mdp.states[np.random.randint(0, mdp.num_states-1)]\n",
    "            else:\n",
    "                net.learn(next_state, reward)\n",
    "                curr_state = next_state\n",
    "\n",
    "    def test_convergence_sgd(self):\n",
    "        dqn = DQN(self.mdp, hidden_dim=128, l2_reg=0.0, epsilon=0.05)\n",
    "        self.train(dqn, 5000)\n",
    "        \n",
    "        # there is a secret \"5-th\" state corresponding to the second visit\n",
    "        # to the absorbing state (to avoid infinite looping), so only check the\n",
    "        # first four states\n",
    "        numpy.testing.assert_almost_equal(self.all_q_vals(dqn)[:4],\n",
    "                                          [[.7, .25], [.35, .5],\n",
    "                                           [.25, 1.0], [0., 0.]], 2)\n",
    "    \n",
    "    def test_pickle(self):\n",
    "        '''\n",
    "            Ensure the model parameters are successfully loaded and unloaded\n",
    "        '''\n",
    "        PATH = 'dqn_pickle_test.cpkl'\n",
    "        dqn_one = DQN(mdp, hidden_dim=128, l2_reg=0.0, epsilon=0.2)\n",
    "        self.train(dqn_one, 10)\n",
    "        \n",
    "        # save params\n",
    "        dqn_one.save_params(PATH)\n",
    "\n",
    "        # initialize a second dqn\n",
    "        dqn_two = DQN(mdp, hidden_dim=128, l2_reg=0.0, epsilon=0.2)\n",
    "\n",
    "        # load params\n",
    "        dqn_two.load_params(PATH)\n",
    "        \n",
    "        os.remove(PATH)\n",
    "\n",
    "        #  Verify that the output values of dqn_one and dqn_two are identical\n",
    "        numpy.testing.assert_almost_equal(self.all_q_vals(dqn_one),self.all_q_vals(dqn_two))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Testing the DQN\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(DQNTest)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####\n",
    "# Unit test for the Recurrent Policy Gradient Implementation\n",
    "#####\n",
    "class ReinforceActionTest(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.mdp = ChainMDP(reward_left=-20, reward_right=20)\n",
    "\n",
    "    def all_action_distribution(self, net):\n",
    "        \"\"\" Helper method to get the entire Q-table \"\"\"\n",
    "\n",
    "        action_probs = np.zeros((self.mdp.num_states, self.mdp.num_actions))\n",
    "        for i in range(self.mdp.num_states):\n",
    "            net.reset_net()\n",
    "            action_probs[i, :] = net.fprop(self.mdp.states[i].T).flatten()\n",
    "        return action_probs\n",
    "\n",
    "    def train(self, net, steps):\n",
    "        mdp = self.mdp\n",
    "        curr_state =mdp.states[np.random.randint(0, mdp.num_states-1)]\n",
    "        for step in xrange(steps):\n",
    "            action = net.get_action(curr_state)\n",
    "            reward, next_state, terminal = mdp.act(curr_state, action)\n",
    "            if terminal:\n",
    "                net.end_episode(reward)\n",
    "                curr_state = mdp.states[np.random.randint(0, mdp.num_states-1)]\n",
    "            else:\n",
    "                net.learn(next_state, reward)\n",
    "                curr_state = next_state\n",
    "\n",
    "    def test_convergence_sgd(self):\n",
    "        rr_agent = RecurrentReinforceAgent(self.mdp, hidden_dim=128, num_samples=10, mode='fast_compile')\n",
    "        self.train(rr_agent, 30000)\n",
    "        diffs = (self.all_action_distribution(rr_agent)[:4, 1] - [0.95]*4) < 0\n",
    "        self.assertEqual(sum(diffs), 0)\n",
    "    \n",
    "    def test_pickle(self):\n",
    "        '''\n",
    "            Ensure the model parameters are successfully loaded and unloaded\n",
    "        '''\n",
    "        PATH = 'rr_agent_pickle_test.cpkl'\n",
    "        rr_agent_one = RecurrentReinforceAgent(self.mdp, hidden_dim=128, num_samples=10, mode='fast_compile')\n",
    "        self.train(rr_agent_one, 20)\n",
    "        \n",
    "        # save params\n",
    "        rr_agent_one.save_params(PATH)\n",
    "\n",
    "        # initialize a second dqn\n",
    "        rr_agent_two = RecurrentReinforceAgent(self.mdp, hidden_dim=128, num_samples=10, mode='fast_compile')\n",
    "\n",
    "        # load params\n",
    "        rr_agent_two.load_params(PATH)\n",
    "        \n",
    "        os.remove(PATH)\n",
    " \n",
    "        #  Verify that the output values of dqn_one and dqn_two are identical\n",
    "        numpy.testing.assert_almost_equal(self.all_action_distribution(rr_agent_one) ,self.all_action_distribution(rr_agent_two))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Testing the Recurrent Reinforce Agent\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(ReinforceActionTest)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
