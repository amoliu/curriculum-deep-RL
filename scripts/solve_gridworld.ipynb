{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scriptinit\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pylab as plt\n",
    "from IPython import display\n",
    "from gridworld import Grid, GridWorldMDP, GridWorld\n",
    "from agent import ValueIterationSolver, TDLearner, DQN, RecurrentReinforceAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up a very simple world with a single wall\n",
    "world = np.zeros((3, 4))\n",
    "world[1, 1] = 1.\n",
    "\n",
    "# 2 reward states each with +/- 1 reward\n",
    "rewards = {(0, 3): 1., (1, 3): -1.}\n",
    "\n",
    "grid = Grid(world, action_stoch=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.74571474  0.84849719  0.96545788  1.        ]\n",
      " [ 0.65555608  0.          0.75177567 -1.        ]\n",
      " [ 0.57964931  0.57734331  0.65267736  0.49373426]]\n",
      "Expected reward 0.696711756054\n"
     ]
    }
   ],
   "source": [
    "# Solve the grid using value iteration as a sanity check\n",
    "mdp = GridWorldMDP(grid, rewards, wall_penalty=0., gamma=0.9)\n",
    "mdp_agent = ValueIterationSolver(mdp, tol=1e-6)\n",
    "mdp_agent.learn()\n",
    "\n",
    "# visualize the results\n",
    "values = np.zeros(world.shape)\n",
    "for state in xrange(grid.get_num_states()):\n",
    "    values[grid.state_pos[state]] = mdp_agent.V[state]\n",
    "\n",
    "expected_reward = np.sum(values) / float(world.shape[0] * world.shape[1] - np.sum(world) - len(rewards))\n",
    "\n",
    "# put the terminal state rewards in for visualization purposes\n",
    "for pos, r in rewards.items():\n",
    "    values[pos] = r\n",
    "\n",
    "print values\n",
    "print 'Expected reward', expected_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# template for interaction between the agent and the task\n",
    "def grid_experiment(agent, task, num_episodes, diagnostic_callback=None, diagnostic_frequency=100):\n",
    "    for episode in xrange(NUM_EPISODES):\n",
    "        total_reward = 0.\n",
    "        while grid_task.is_terminal():\n",
    "            grid_task.reset()\n",
    "\n",
    "        curr_state = grid_task.get_current_state()\n",
    "        num_steps = 0.\n",
    "        while True:\n",
    "            num_steps += 1\n",
    "            if num_steps > 200:\n",
    "                print 'Lying and tell the agent the episode is over!'\n",
    "                agent.end_episode(0)\n",
    "                num_steps = 0.\n",
    "\n",
    "            action = agent.get_action(curr_state)\n",
    "            next_state, reward = grid_task.perform_action(action)\n",
    "            total_reward += reward\n",
    "            if grid_task.is_terminal():\n",
    "                agent.end_episode(reward)\n",
    "                break\n",
    "            else:\n",
    "                agent.learn(next_state, reward)\n",
    "                curr_state = next_state\n",
    "\n",
    "        if episode % diagnostic_frequency == 0:\n",
    "            if diagnostic_callback is not None:\n",
    "                diagnostic_callback(episode, total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling fprop\n",
      "Compiling backprop\n",
      "done\n",
      "Episode number:  0\n",
      "[[ 0.34996574  0.08153344  0.1840996   1.        ]\n",
      " [ 0.          0.          0.19193913 -1.        ]\n",
      " [ 0.39006751  0.47662856  0.04981069  0.        ]] \n",
      "\n",
      "Lying and tell the agent the episode is over!\n",
      "Lying and tell the agent the episode is over!\n",
      "Episode number:  1000\n",
      "[[ 0.71301916  0.83421038  0.96474456  1.        ]\n",
      " [ 0.47287326  0.          0.86819273 -1.        ]\n",
      " [ 0.42694322  0.41502009  0.47287326  0.47287326]] \n",
      "\n",
      "Episode number:  2000\n",
      "[[ 0.75148717  0.87401367  0.97688674  1.        ]\n",
      " [ 0.32268025  0.          0.69442223 -1.        ]\n",
      " [ 0.32697054  0.32384039  0.32268025  0.32268025]] \n",
      "\n",
      "Lying and tell the agent the episode is over!\n",
      "Episode number:  3000\n",
      "[[ 0.53477335  0.77994617  0.96175371  1.        ]\n",
      " [ 0.15459601  0.          0.65229152 -1.        ]\n",
      " [ 0.18126212  0.28192748  0.35258198  0.15459601]] \n",
      "\n",
      "Episode number:  4000\n",
      "[[ 0.29612517  0.77450618  0.97078216  1.        ]\n",
      " [ 0.10310403  0.          0.40341919 -1.        ]\n",
      " [ 0.18609466  0.19609053  0.19815601  0.10310403]] \n",
      "\n",
      "Episode number:  5000\n",
      "[[ 0.47485593  0.8654041   0.94738259  1.        ]\n",
      " [ 0.18243666  0.          0.49392182 -1.        ]\n",
      " [ 0.33779484  0.40080937  0.39664177  0.18243666]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solve gridworld using a DQN\n",
    "\n",
    "# setup task and agent\n",
    "NUM_EPISODES = 5001\n",
    "grid_task = GridWorld(grid, rewards, wall_penalty=0., gamma=0.9, tabular=False)\n",
    "dqn = DQN(grid_task, hidden_dim=128, l2_reg=0.0, lr=0.05, epsilon=0.15)\n",
    "\n",
    "# diagnostic function\n",
    "def compute_value_function(episode, total_reward):\n",
    "    print 'Episode number: ',  episode\n",
    "    values = np.zeros(world.shape)\n",
    "    for row in xrange(world.shape[0]):\n",
    "        for col in xrange(world.shape[1]):\n",
    "            if world[row, col] == 0:  # agent can occupy this state\n",
    "                agent_state = np.zeros_like(world)\n",
    "                agent_state[row, col] = 1.\n",
    "                state = agent_state.ravel().reshape(-1, 1)\n",
    "\n",
    "                qvals = dqn.fprop(state.T)\n",
    "                values[row, col] = np.max(qvals)\n",
    "\n",
    "    for pos, r in rewards.items():\n",
    "        values[pos] = r\n",
    "\n",
    "    print values, '\\n'\n",
    "\n",
    "# run the experiment\n",
    "grid_experiment(dqn, grid_task, NUM_EPISODES, diagnostic_callback=compute_value_function, diagnostic_frequency=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solve gridworld using the recurrent reinforce agent (policy gradient)\n",
    "\n",
    "# setup new task and agent\n",
    "NUM_EPISODES = 10001\n",
    "grid = Grid(world, action_stoch=0.2)\n",
    "grid_task = GridWorld(grid, rewards, wall_penalty=0., gamma=0.9, tabular=False)\n",
    "rr_agent = RecurrentReinforceAgent(grid_task, hidden_dim=128, num_samples=10)\n",
    "\n",
    "\n",
    "# define diagnostic function (maintains internal state)\n",
    "class plot_historical_avg():\n",
    "    def __init__(self):\n",
    "#         self.running_avg_reward = 0.\n",
    "#         self.avg_reward_hist = []\n",
    "        self.reward_hist = []\n",
    "        self.moving_avg_reward = []\n",
    "        self.episodes = []\n",
    "    \n",
    "    def __call__(self, episode, total_reward):\n",
    "#         self.running_avg_reward += (1. / (episode + 1)) * (total_reward - self.running_avg_reward)\n",
    "#         self.avg_reward_hist.append(self.running_avg_reward)\n",
    "        self.reward_hist.append(total_reward)\n",
    "        if episode % 50 == 0:\n",
    "            self.moving_avg_reward.append(np.mean(self.reward_hist))\n",
    "            # clear history\n",
    "            self.reward_hist = []\n",
    "            self.episodes.append(episode)\n",
    "           \n",
    "            # plotting\n",
    "            plt.plot(self.episodes, expected_reward * np.ones(len(self.episodes)), 'r')\n",
    "            plt.plot(self.episodes, self.moving_avg_reward, 'b')\n",
    "            plt.xlabel('Iterations')\n",
    "            plt.ylabel('Average Reward')\n",
    "            plt.title('Average Reward over The Last 30 time steps versus Iteration')\n",
    "            display.display(plt.gcf())\n",
    "#             display.clear_output(wait=True)\n",
    "\n",
    "    \n",
    "historical_averager = plot_historical_avg()\n",
    "\n",
    "# run the experiment!\n",
    "grid_experiment(rr_agent, grid_task, NUM_EPISODES, diagnostic_callback=historical_averager, diagnostic_frequency=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
