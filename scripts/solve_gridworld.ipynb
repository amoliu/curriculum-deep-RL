{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scriptinit\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pylab as plt\n",
    "from IPython import display\n",
    "from gridworld import Grid, GridWorldMDP, GridWorld\n",
    "from agent import ValueIterationSolver, TDLearner, DQN, RecurrentReinforceAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up a very simple world with a single wall\n",
    "world = np.zeros((3, 4))\n",
    "world[1, 1] = 1.\n",
    "\n",
    "# 2 reward states each with +/- 1 reward\n",
    "rewards = {(0, 3): 1., (1, 3): -1.}\n",
    "\n",
    "grid = Grid(world, action_stoch=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solve the grid using value iteration as a sanity check\n",
    "mdp = GridWorldMDP(grid, rewards, wall_penalty=0., gamma=0.9)\n",
    "mdp_agent = ValueIterationSolver(mdp, tol=1e-6)\n",
    "mdp_agent.learn()\n",
    "\n",
    "# visualize the results\n",
    "values = np.zeros(world.shape)\n",
    "for state in xrange(grid.get_num_states()):\n",
    "    values[grid.state_pos[state]] = mdp_agent.V[state]\n",
    "\n",
    "expected_reward = np.sum(values) / float(world.shape[0] * world.shape[1] - np.sum(world) - len(rewards))\n",
    "\n",
    "# put the terminal state rewards in for visualization purposes\n",
    "for pos, r in rewards.items():\n",
    "    values[pos] = r\n",
    "\n",
    "print values\n",
    "print 'Expected reward', expected_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# template for interaction between the agent and the task\n",
    "def grid_experiment(agent, task, num_episodes, diagnostic_callback=None, diagnostic_frequency=100):\n",
    "    for episode in xrange(NUM_EPISODES):\n",
    "        total_reward = 0.\n",
    "        while grid_task.is_terminal():\n",
    "            grid_task.reset()\n",
    "\n",
    "        curr_state = grid_task.get_current_state()\n",
    "        num_steps = 0.\n",
    "        while True:\n",
    "            num_steps += 1\n",
    "            if num_steps > 200:\n",
    "                print 'Lying and tell the agent the episode is over!'\n",
    "                agent.end_episode(0)\n",
    "                num_steps = 0.\n",
    "\n",
    "            action = agent.get_action(curr_state)\n",
    "            next_state, reward = grid_task.perform_action(action)\n",
    "            total_reward += reward\n",
    "            if grid_task.is_terminal():\n",
    "                agent.end_episode(reward)\n",
    "                break\n",
    "            else:\n",
    "                agent.learn(next_state, reward)\n",
    "                curr_state = next_state\n",
    "\n",
    "        if episode % diagnostic_frequency == 0:\n",
    "            if diagnostic_callback is not None:\n",
    "                diagnostic_callback(episode, total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solve gridworld using a DQN\n",
    "\n",
    "# setup task and agent\n",
    "NUM_EPISODES = 5001\n",
    "grid_task = GridWorld(grid, rewards, wall_penalty=0., gamma=0.9, tabular=False)\n",
    "dqn = DQN(grid_task, hidden_dim=128, l2_reg=0.0, lr=0.05, epsilon=0.15)\n",
    "\n",
    "# diagnostic function\n",
    "def compute_value_function(episode, total_reward):\n",
    "    print 'Episode number: ',  episode\n",
    "    values = np.zeros(world.shape)\n",
    "    for row in xrange(world.shape[0]):\n",
    "        for col in xrange(world.shape[1]):\n",
    "            if world[row, col] == 0:  # agent can occupy this state\n",
    "                agent_state = np.zeros_like(world)\n",
    "                agent_state[row, col] = 1.\n",
    "                state = agent_state.ravel().reshape(-1, 1)\n",
    "\n",
    "                qvals = dqn.fprop(state.T)\n",
    "                values[row, col] = np.max(qvals)\n",
    "\n",
    "    for pos, r in rewards.items():\n",
    "        values[pos] = r\n",
    "\n",
    "    print values, '\\n'\n",
    "\n",
    "# run the experiment\n",
    "grid_experiment(dqn, grid_task, NUM_EPISODES, diagnostic_callback=compute_value_function, diagnostic_frequency=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solve gridworld using the recurrent reinforce agent (policy gradient)\n",
    "\n",
    "# setup new task and agent\n",
    "NUM_EPISODES = 10001\n",
    "grid = Grid(world, action_stoch=0.2)\n",
    "grid_task = GridWorld(grid, rewards, wall_penalty=0., gamma=0.9, tabular=False)\n",
    "rr_agent = RecurrentReinforceAgent(grid_task, hidden_dim=128, num_samples=10)\n",
    "\n",
    "\n",
    "# define diagnostic function (maintains internal state)\n",
    "class plot_historical_avg():\n",
    "    def __init__(self):\n",
    "#         self.running_avg_reward = 0.\n",
    "#         self.avg_reward_hist = []\n",
    "        self.reward_hist = []\n",
    "        self.moving_avg_reward = []\n",
    "        self.episodes = []\n",
    "    \n",
    "    def __call__(self, episode, total_reward):\n",
    "#         self.running_avg_reward += (1. / (episode + 1)) * (total_reward - self.running_avg_reward)\n",
    "#         self.avg_reward_hist.append(self.running_avg_reward)\n",
    "        self.reward_hist.append(total_reward)\n",
    "        if episode % 50 == 0:\n",
    "            self.moving_avg_reward.append(np.mean(self.reward_hist))\n",
    "            # clear history\n",
    "            self.reward_hist = []\n",
    "            self.episodes.append(episode)\n",
    "           \n",
    "            # plotting\n",
    "            plt.plot(self.episodes, expected_reward * np.ones(len(self.episodes)), 'r')\n",
    "            plt.plot(self.episodes, self.moving_avg_reward, 'b')\n",
    "            plt.xlabel('Iterations')\n",
    "            plt.ylabel('Average Reward')\n",
    "            plt.title('Average Reward over The Last 30 time steps versus Iteration')\n",
    "            display.display(plt.gcf())\n",
    "#             display.clear_output(wait=True)\n",
    "\n",
    "    \n",
    "historical_averager = plot_historical_avg()\n",
    "\n",
    "# run the experiment!\n",
    "grid_experiment(rr_agent, grid_task, NUM_EPISODES, diagnostic_callback=historical_averager, diagnostic_frequency=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
