{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve the GridWorld problems with goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a MazeWorld problems. \n",
    "\n",
    "Assume we have a series of tasks. Every task share the same $H \\times W$ maze, but has a different goal. <br>\n",
    "The goal is to navigate and collect a bean. \n",
    "\n",
    "The agent knows aprior the shape of the maze, as well as the location of the bean encoded as a $H \\times W$ 0-1 matrix.\n",
    "\n",
    "**Question**\n",
    "\n",
    "1. Is it possible for the agent to learn a shared Q network for different tasks?\n",
    "    > YES\n",
    "    \n",
    "    * note: \n",
    "    \n",
    "    **It is better to have the training tasks interleave. i.e. one task show up for training for only a small number of episodes**\n",
    "\n",
    "\n",
    "\n",
    "2. Is it possible to learn Q network on subset of the tasks, and generalize to others?\n",
    "\n",
    "    * Idea 1. Set goals only at even locations, i.e.\n",
    "    $$G[2 i, 2 j] = 1$$ for some $i$ and $j$.\n",
    "    And test at tasks with other locations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imported Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import scriptinit\n",
    "from scriptutils import *\n",
    "import numpy as np\n",
    "import numpy.linalg as npla\n",
    "import numpy.random as npr\n",
    "import matplotlib\n",
    "import pylab as plt\n",
    "import time\n",
    "import pdb\n",
    "from IPython import display\n",
    "from gridworld import Grid, GridWorldMDP, GridWorld, GridWorldWithGoals\n",
    "from agent import ValueIterationSolver, TDLearner, DQN, RecurrentReinforceAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reused Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# diagnostic function\n",
    "def compute_value_function(goal, rewards, total_reward):\n",
    "    global GT\n",
    "    global grid\n",
    "    values = np.zeros(world.shape)\n",
    "    dqn.task.reset(goal, rewards)\n",
    "    for row in xrange(world.shape[0]):\n",
    "        for col in xrange(world.shape[1]):\n",
    "            if world[row, col] == 0:  # agent can occupy this state\n",
    "                agent_state = np.zeros_like(world)\n",
    "                agent_state[row, col] = 1.\n",
    "                state_resized = agent_state.ravel().reshape(-1, 1)\n",
    "                goal_resized = goal.reshape(-1, 1)\n",
    "                state = np.concatenate((state_resized, goal_resized), axis=0)\n",
    "                qvals = dqn.fprop(state.T)\n",
    "                values[row, col] = np.max(qvals)\n",
    "    \n",
    "    for pos, r in rewards.items():\n",
    "        values[pos] = r\n",
    "\n",
    "    print 'values', values, '\\n'\n",
    "    \n",
    "    print 'goal', dqn.task.goal, '\\n'\n",
    "    \n",
    "#     GT = solve_by_value_iteration(grid, rewards)\n",
    "#     error = npla.norm(values - GT, 'fro')\n",
    "#     return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# template for interaction between the agent and the task\n",
    "def grid_experiment(agent, grid_task, goal_rewards_pairs, num_episodes, num_episode_in_task, diagnostic_callback=None, switch_task_frequency=100, diagnostic_frequency=100):\n",
    "    episodes = []\n",
    "    errors = []\n",
    "    (goal, rewards) = goal_rewards_pairs[0]\n",
    "    for episode in xrange(num_episodes):\n",
    "        total_reward = 0.\n",
    "        while grid_task.is_terminal():\n",
    "            ind = npr.choice(len(goal_rewards_pairs), 1)\n",
    "            if episode % num_episode_in_task == 0:\n",
    "                (goal, rewards) = goal_rewards_pairs[ind]\n",
    "            grid_task.reset(goal, rewards)\n",
    "\n",
    "        curr_state = grid_task.get_current_state()\n",
    "        num_steps = 0.\n",
    "        while True:\n",
    "            num_steps += 1\n",
    "            if num_steps > 200:\n",
    "                print 'Lying and tell the agent the episode is over!'\n",
    "                agent.end_episode(0)\n",
    "                num_steps = 0.\n",
    "            action = agent.get_action(curr_state)\n",
    "            next_state, reward = grid_task.perform_action(action)\n",
    "#             print 'state', grid_task.env.state_pos[grid_task.env.get_current_state()]\n",
    "            total_reward += reward\n",
    "            if grid_task.is_terminal():\n",
    "                agent.end_episode(reward)\n",
    "                break\n",
    "            else:\n",
    "                agent.learn(next_state, reward)\n",
    "                curr_state = next_state\n",
    "        \n",
    "        if episode % diagnostic_frequency == 0:\n",
    "            if diagnostic_callback is not None:\n",
    "                print 'episode number: ',  episode\n",
    "                error = diagnostic_callback(goal, rewards, total_reward)\n",
    "                episodes.append(episode)\n",
    "                errors.append(error)\n",
    "                \n",
    "    # plot convergence curve.\n",
    "#     if errors:\n",
    "#         plt.cla()\n",
    "#         plt.plot(episodes, errors)\n",
    "        # display.clear_output(wait=True)\n",
    "#         display.display(plt.gcf())\n",
    "#         plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x107b894d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVkAAAEACAYAAAAZcwXkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nay32AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107add9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set up the environment.\n",
    "H = 3\n",
    "W = 4\n",
    "world = np.zeros((H, W))\n",
    "\n",
    "goal_rewards_pairs = []\n",
    "for ni in range(0, H, 2):\n",
    "    for nj in range(0, W, 2):\n",
    "        goal = np.zeros((H, W))\n",
    "        goal[ni, nj] = 1.\n",
    "        rewards = {(ni, nj): 1.}\n",
    "        goal_rewards_pairs.append((goal, rewards))\n",
    "(base_goal, base_rewards) = goal_rewards_pairs[0]\n",
    "task = GridWorldWithGoals(grid, base_goal, base_rewards, wall_penalty=0., gamma=0.9)\n",
    "_ = plt.imshow(solve_world_by_value_iteration(world, base_rewards), interpolation='none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "** Use 2 hidden layers, each with 128 dimension**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling fprop\n",
      "Compiling backprop\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# deep Q learning.\n",
    "dqn = DQN(task, hidden_dim=128, l2_reg=0.0, lr=0.05, epsilon=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lying and tell the agent the episode is over!\n",
      "episode number:  0\n",
      "values [[ 1.          0.39503204  0.41770889  0.43323954]\n",
      " [ 0.62324082  0.409072    0.35109647  0.37036856]\n",
      " [ 0.44396185  0.42783675  0.37060292  0.41106561]] \n",
      "\n",
      "goal [[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  1000\n",
      "values [[ 0.7793838   0.75884418  0.84264804  0.70927523]\n",
      " [ 0.73775788  0.87430068  0.89976754  0.84525264]\n",
      " [ 0.84475463  0.95881654  1.          0.90548252]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]] \n",
      "\n",
      "episode number:  2000\n",
      "values [[ 1.          0.9378785   0.81707946  0.72770295]\n",
      " [ 1.03033269  0.9012481   0.78863645  0.6831857 ]\n",
      " [ 0.89155016  0.74360622  0.66712954  0.59576336]] \n",
      "\n",
      "goal [[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  3000\n",
      "values [[ 1.          0.97586961  0.87658087  0.76384106]\n",
      " [ 0.95333487  0.87633166  0.71075993  0.64713874]\n",
      " [ 0.78846159  0.77658813  0.69440406  0.60955361]] \n",
      "\n",
      "goal [[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  4000\n",
      "values [[ 0.87244347  0.95451986  1.          0.97911811]\n",
      " [ 0.76824684  0.8828409   0.9579945   0.83525561]\n",
      " [ 0.66853043  0.78847991  0.84025264  0.81399286]] \n",
      "\n",
      "goal [[ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  5000\n",
      "values [[ 0.85659864  0.96725177  1.          0.98076236]\n",
      " [ 0.77073854  0.85514765  0.91324479  0.87066937]\n",
      " [ 0.7138017   0.79851114  0.78567291  0.80447482]] \n",
      "\n",
      "goal [[ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  6000\n",
      "values [[ 0.849213    0.96059325  1.          0.97201261]\n",
      " [ 0.80240172  0.87668362  1.00423985  0.88335138]\n",
      " [ 0.7296358   0.79640094  0.89649332  0.75222646]] \n",
      "\n",
      "goal [[ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  7000\n",
      "values [[ 0.65497228  0.73757713  0.77062801  0.6973132 ]\n",
      " [ 0.74011163  0.84373562  0.87394232  0.80630815]\n",
      " [ 0.84637491  0.98383613  1.          0.9972513 ]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]] \n",
      "\n",
      "episode number:  8000\n",
      "values [[ 1.          0.97756595  0.87198333  0.7717106 ]\n",
      " [ 0.97202574  0.88055068  0.76395145  0.67223392]\n",
      " [ 0.85897542  0.77438899  0.70284984  0.63983549]] \n",
      "\n",
      "goal [[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  9000\n",
      "values [[ 0.80475659  0.75907381  0.70245654  0.64873399]\n",
      " [ 0.96621999  0.86856201  0.78121832  0.67765726]\n",
      " [ 1.          0.9489374   0.81366179  0.71166937]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]] \n",
      "\n",
      "episode number:  10000\n",
      "values [[ 0.87430781  0.96884139  1.          0.9855349 ]\n",
      " [ 0.7840595   0.87400278  0.96919996  0.88158309]\n",
      " [ 0.6992575   0.75123968  0.85824622  0.78706654]] \n",
      "\n",
      "goal [[ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  11000\n",
      "values [[ 0.72036747  0.82890194  0.91487797  0.80688932]\n",
      " [ 0.76121135  0.88909543  1.02105911  0.904961  ]\n",
      " [ 0.81247254  0.88048238  1.          1.0085588 ]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]] \n",
      "\n",
      "episode number:  12000\n",
      "values [[ 0.92102373  1.02234414  1.          0.91037506]\n",
      " [ 0.77847117  0.86089002  1.01508542  0.95710068]\n",
      " [ 0.72120316  0.81573996  0.90349454  0.81225587]] \n",
      "\n",
      "goal [[ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  13000\n",
      "values [[ 0.65921838  0.75681524  0.85401573  0.76928821]\n",
      " [ 0.7398454   0.85210657  0.98138644  0.86495166]\n",
      " [ 0.86270624  0.95480153  1.          0.9701641 ]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]] \n",
      "\n",
      "episode number:  14000\n",
      "values [[ 0.67798126  0.75671099  0.83518143  0.76198606]\n",
      " [ 0.74006458  0.81350647  0.93229079  0.86246143]\n",
      " [ 0.80717211  0.94727153  1.          0.96797483]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]] \n",
      "\n",
      "episode number:  15000\n",
      "values [[ 0.84543675  0.97059489  1.          0.99422706]\n",
      " [ 0.78135339  0.88750244  0.98946642  0.90169424]\n",
      " [ 0.72277881  0.80504621  0.85149919  0.8081312 ]] \n",
      "\n",
      "goal [[ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  16000\n",
      "values [[ 0.82751103  0.79794418  0.69656132  0.63464811]\n",
      " [ 0.95581837  0.91191326  0.76848353  0.69977462]\n",
      " [ 1.          1.008344    0.85186576  0.78923443]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]] \n",
      "\n",
      "episode number:  17000\n",
      "values [[ 0.88622885  0.7950787   0.6843411   0.62264341]\n",
      " [ 1.00474645  0.88277711  0.71793187  0.63409937]\n",
      " [ 1.          1.00909288  0.84705725  0.71757032]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]] \n",
      "\n",
      "episode number:  18000\n",
      "values [[ 0.8187576   0.92145175  1.          1.00105788]\n",
      " [ 0.77695547  0.86004697  0.96211819  0.90854851]\n",
      " [ 0.6995658   0.77059711  0.87737375  0.81764306]] \n",
      "\n",
      "goal [[ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  19000\n",
      "values [[ 1.          0.99669546  0.85872262  0.77551956]\n",
      " [ 0.98702368  0.86285964  0.74310293  0.66981571]\n",
      " [ 0.88706958  0.79041578  0.7087591   0.6337809 ]] \n",
      "\n",
      "goal [[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  20000\n",
      "values [[ 0.82767684  0.76500503  0.68931279  0.62599804]\n",
      " [ 0.96935534  0.87615593  0.72069343  0.65950766]\n",
      " [ 1.          0.91737703  0.82124137  0.74148339]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]] \n",
      "\n",
      "episode number:  21000\n",
      "values [[ 0.88213585  0.78748495  0.70937744  0.6403007 ]\n",
      " [ 0.98152332  0.89133134  0.78611134  0.70925678]\n",
      " [ 1.          0.99903924  0.87780792  0.69134266]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]] \n",
      "\n",
      "episode number:  22000\n",
      "values [[ 1.          0.95132138  0.83597649  0.74961974]\n",
      " [ 0.91639905  0.82912649  0.73493726  0.66839086]\n",
      " [ 0.81626862  0.74142659  0.67507407  0.60781412]] \n",
      "\n",
      "goal [[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  23000\n",
      "values [[ 0.66768262  0.71596249  0.82768519  0.72016045]\n",
      " [ 0.7534477   0.87210215  0.94885107  0.86612267]\n",
      " [ 0.84910282  0.99019285  1.          1.01081306]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]] \n",
      "\n",
      "episode number:  24000\n",
      "values [[ 1.          0.99408949  0.88387371  0.78048515]\n",
      " [ 0.970319    0.8636532   0.77428228  0.6795677 ]\n",
      " [ 0.84344888  0.77201948  0.67994549  0.61294606]] \n",
      "\n",
      "goal [[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  25000\n",
      "values [[ 0.86556345  0.77133791  0.6903897   0.63408231]\n",
      " [ 0.98629622  0.80596063  0.76644719  0.6093956 ]\n",
      " [ 1.          0.88745577  0.80438754  0.64510814]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]] \n",
      "\n",
      "episode number:  26000\n",
      "values [[ 0.75131829  0.88735455  1.          0.96347963]\n",
      " [ 0.71148429  0.79832557  0.93016268  0.84693591]\n",
      " [ 0.66195323  0.71837828  0.81037489  0.74276836]] \n",
      "\n",
      "goal [[ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  27000\n",
      "values [[ 0.87628563  0.79682158  0.71503821  0.62116402]\n",
      " [ 0.9903465   0.85682349  0.73364577  0.66253615]\n",
      " [ 1.          0.99610538  0.85185893  0.76019506]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]] \n",
      "\n",
      "episode number:  28000\n",
      "values [[ 1.          0.96703012  0.85078685  0.7674613 ]\n",
      " [ 0.98595395  0.85835263  0.77244038  0.69392162]\n",
      " [ 0.87470242  0.76767827  0.67816602  0.62874526]] \n",
      "\n",
      "goal [[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  29000\n",
      "values [[ 0.86494209  0.78243316  0.70683942  0.64886856]\n",
      " [ 0.98135613  0.8836045   0.76589307  0.69542744]\n",
      " [ 1.          0.99315006  0.87360203  0.79355385]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]] \n",
      "\n",
      "episode number:  30000\n",
      "values [[ 0.67671484  0.74277337  0.83684186  0.78116691]\n",
      " [ 0.75709157  0.82287997  0.93277696  0.8739646 ]\n",
      " [ 0.82725062  0.94147424  1.          1.00339848]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]] \n",
      "\n",
      "episode number:  31000\n",
      "values [[ 1.          0.9867425   0.86450685  0.76093463]\n",
      " [ 0.94790646  0.85410443  0.76752166  0.6853896 ]\n",
      " [ 0.82858684  0.74067625  0.68846047  0.61222762]] \n",
      "\n",
      "goal [[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  32000\n",
      "values [[ 1.          0.99117556  0.85296994  0.7531535 ]\n",
      " [ 0.97821799  0.86780499  0.7831478   0.70298314]\n",
      " [ 0.87707556  0.78187483  0.69208111  0.649202  ]] \n",
      "\n",
      "goal [[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  33000\n",
      "values [[ 1.          0.96214039  0.83169683  0.70814732]\n",
      " [ 0.93382086  0.8401378   0.73842857  0.64480177]\n",
      " [ 0.84297853  0.75165872  0.66268777  0.59350933]] \n",
      "\n",
      "goal [[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  34000\n",
      "values [[ 0.8257649   0.97780934  1.          0.99848635]\n",
      " [ 0.76738636  0.86223907  0.97735641  0.90487236]\n",
      " [ 0.70170706  0.77702048  0.848427    0.7965922 ]] \n",
      "\n",
      "goal [[ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  35000\n",
      "values [[ 1.          0.99957755  0.86915949  0.71755486]\n",
      " [ 0.88425395  0.85111582  0.75747018  0.66903648]\n",
      " [ 0.78823888  0.73643178  0.67244445  0.59811067]] \n",
      "\n",
      "goal [[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  36000\n",
      "values [[ 1.          0.97830593  0.81271815  0.72571934]\n",
      " [ 0.94972803  0.86916651  0.6924616   0.65394763]\n",
      " [ 0.85453551  0.77013649  0.6447696   0.58963769]] \n",
      "\n",
      "goal [[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  37000\n",
      "values [[ 0.68211841  0.73628905  0.87770844  0.81397141]\n",
      " [ 0.74119371  0.85835245  0.99588408  0.89612739]\n",
      " [ 0.82986386  0.95537458  1.          0.99524976]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]] \n",
      "\n",
      "episode number:  38000\n",
      "values [[ 1.          0.96972941  0.85843223  0.78073136]\n",
      " [ 0.95898306  0.87455403  0.72239392  0.67429471]\n",
      " [ 0.86698248  0.7790645   0.7041413   0.62275934]] \n",
      "\n",
      "goal [[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  39000\n",
      "values [[ 0.76393599  0.7117652   0.65583352  0.59071998]\n",
      " [ 0.92829199  0.82197449  0.72910901  0.7004279 ]\n",
      " [ 1.          0.97389201  0.84771668  0.7737764 ]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]] \n",
      "\n",
      "episode number:  40000\n",
      "values [[ 0.83469701  0.74817772  0.66337551  0.54786782]\n",
      " [ 0.99382852  0.86387821  0.76009358  0.68675433]\n",
      " [ 1.          0.98319908  0.84815719  0.76002967]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]] \n",
      "\n",
      "episode number:  41000\n",
      "values [[ 0.6825034   0.75684943  0.78590324  0.75384381]\n",
      " [ 0.74321238  0.84925869  0.9657656   0.83303444]\n",
      " [ 0.84205663  0.94353781  1.          0.96418115]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]] \n",
      "\n",
      "episode number:  42000\n",
      "values [[ 0.65419362  0.76090085  0.84612208  0.79971612]\n",
      " [ 0.74125649  0.84811661  0.9849728   0.89678094]\n",
      " [ 0.83485761  0.95217248  1.          0.95963194]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]] \n",
      "\n",
      "episode number:  43000\n",
      "values [[ 0.78972295  0.72811022  0.63868255  0.60055739]\n",
      " [ 0.92298667  0.80425863  0.72435947  0.62773522]\n",
      " [ 1.          0.96511044  0.81511443  0.6799441 ]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]] \n",
      "\n",
      "episode number:  44000\n",
      "values [[ 0.66261761  0.7320874   0.81734493  0.7417104 ]\n",
      " [ 0.76757265  0.81777605  0.9193443   0.87518804]\n",
      " [ 0.85519817  0.96889423  1.          0.99262816]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]] \n",
      "\n",
      "episode number:  45000\n",
      "values [[ 0.88315173  0.77608385  0.69557052  0.61129444]\n",
      " [ 0.99113194  0.89263798  0.80788174  0.68633965]\n",
      " [ 1.          0.98345502  0.86673201  0.76048494]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]] \n",
      "\n",
      "episode number:  46000\n",
      "values [[ 0.77788649  0.9726202   1.          0.95022072]\n",
      " [ 0.77038461  0.84085296  0.9279681   0.85106543]\n",
      " [ 0.66402182  0.75806784  0.82067756  0.70679327]] \n",
      "\n",
      "goal [[ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  47000\n",
      "values [[ 0.83754941  0.73616753  0.66536664  0.58674772]\n",
      " [ 0.96681789  0.84658532  0.76708841  0.66848545]\n",
      " [ 1.          0.97344912  0.86432005  0.710488  ]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]] \n",
      "\n",
      "episode number:  48000\n",
      "values [[ 0.84232677  0.94986335  1.          0.9989162 ]\n",
      " [ 0.78840923  0.88032566  0.9807616   0.90246712]\n",
      " [ 0.71027972  0.78531334  0.86123404  0.82122228]] \n",
      "\n",
      "goal [[ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "episode number:  49000\n",
      "values [[ 0.66726569  0.72419504  0.79976563  0.70964773]\n",
      " [ 0.75721374  0.83584669  0.92695489  0.80900979]\n",
      " [ 0.83393979  0.96800792  1.          0.97520449]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]] \n",
      "\n",
      "episode number:  50000\n",
      "values [[ 0.86187626  0.97304183  1.          0.98784695]\n",
      " [ 0.73802431  0.84072015  0.97387183  0.88995529]\n",
      " [ 0.63509165  0.73782998  0.84021049  0.78909551]] \n",
      "\n",
      "goal [[ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:11: DeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "grid_experiment(dqn, task, goal_rewards_pairs, num_episodes=50001, num_episode_in_task=11, diagnostic_callback=compute_value_function, diagnostic_frequency=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test the learned Q network  aginst ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**on training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values [[ 0.67949661  0.77204059  0.83103233  0.79131052]\n",
      " [ 0.72368814  0.88753414  0.98677481  0.88524093]\n",
      " [ 0.8593017   0.97000265  1.          0.9832392 ]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]] \n",
      "\n",
      "ground truth [[ 0.67282207  0.75552356  0.84508353  0.76824471]\n",
      " [ 0.75590759  0.85388134  0.96534012  0.86414893]\n",
      " [ 0.85323349  0.97049232  1.          0.97679857]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "goal = np.zeros((H, W))\n",
    "goal[2, 2] = 1\n",
    "rewards = {(2,2): 1}\n",
    "compute_value_function(goal, rewards, None)\n",
    "_ = solve_by_value_iteration(grid, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values [[ 1.          0.98058428  0.84217933  0.74199983]\n",
      " [ 0.97823695  0.86778606  0.75809714  0.67054705]\n",
      " [ 0.82504406  0.74715409  0.66159337  0.61395757]] \n",
      "\n",
      "goal [[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "ground truth [[ 1.          0.97023492  0.84774168  0.74555131]\n",
      " [ 0.97049335  0.85391016  0.75141657  0.66509466]\n",
      " [ 0.85322621  0.75574253  0.66883632  0.59515284]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "goal = np.zeros((H, W))\n",
    "goal[0, 0] = 1\n",
    "rewards = {(0,0): 1}\n",
    "compute_value_function(goal, rewards, None)\n",
    "_ = solve_by_value_iteration(grid, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**on test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values [[ 0.84064441  0.84124972  0.77905923  0.81954755]\n",
      " [ 0.75384143  0.75960191  0.73671204  1.        ]\n",
      " [ 0.64726839  0.70468863  0.77843751  0.7130822 ]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "ground truth [[ 0.67840874  0.76362697  0.86393412  0.97678795]\n",
      " [ 0.73727178  0.84057222  0.96557982  1.        ]\n",
      " [ 0.67840864  0.76362696  0.86393412  0.97678795]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "goal = np.zeros((H, W))\n",
    "goal[1, 3] = 1\n",
    "rewards = {(1,3): 1}\n",
    "compute_value_function(goal, rewards, None)\n",
    "_ = solve_by_value_iteration(grid, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values [[ 0.78879935  0.85324834  0.7374074   0.84768598]\n",
      " [ 0.76705193  1.          0.8417424   0.77183193]\n",
      " [ 0.71568966  0.91557017  0.80940718  0.86574871]] \n",
      "\n",
      "goal [[ 0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.]] \n",
      "\n",
      "ground truth [[ 0.86477371  0.97130416  0.85956992  0.76440962]\n",
      " [ 0.97154936  1.          0.96540303  0.84537192]\n",
      " [ 0.86477358  0.97130414  0.85956989  0.76440956]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "goal = np.zeros((H, W))\n",
    "goal[1, 1] = 1\n",
    "rewards = {(1, 1): 1}\n",
    "compute_value_function(goal, rewards, None)\n",
    "_ = solve_by_value_iteration(grid, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using *\"interleaving\"*, the agent could learn much better on training tasks. \n",
    "* this kind of contradicts what John has discovered, i.e. training for long time on a single task is better.\n",
    "* On test tasks, the value function is not accurate. but it seems the agent is able to capture *relative importance* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
